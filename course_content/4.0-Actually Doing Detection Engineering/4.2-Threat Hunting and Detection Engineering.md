# 4.2 :: Threat Hunting and Detection Engineering

Threat hunting is a massive topic and yet another course in itself. While we won't be digging deep into what threat hunting is and how it is done, I can define the practice of Threat Hunting into a single phase: "Huh, that looks weird."

Being able to look at logs and find the "weird" elements is an art and a skill that you will develop with time and practice. Today we have it easy: we know exactly what is weird because we know exactly what executed in our environment. Now that you've uploaded the logs into your Splunk instances, take the time to dig around a little bit. Make sure the fields were properly extracted and that the data was parsed in a way that makes sense (this should be automaticly done by Splunk when you upload them, but I've seen Splunk do some weird things). Pull out some specific fields like the CommandLine fields or the Description fields and see if you can find malicious behavior. If you cannot, that is okay! The point of this exercise is to just get used to looking at logs. We will be implementing some Sigma rules to actually root out the evil in these logs.

## Putting Our Automation To The Test

By now you should have a good understanding of pySigma pipelines and the automation that we've built together to pull rules from your GitHub repo and supply Splunk with the instructions on how to schedule searches. We can now close the loop and see how those scheduled searches perform against live data. Let's start with a review to make sure our infrastructure is prepared for this test.

First things first: confirm that you have the sigma rules we imported in Chapter 3 in the `/rules/production` folder. **TODO** add those rules to that chapter

Next we need to make sure our pipeline is prepared. In the folder `/backend/pipelines/` ensure that you have the pipeline.yaml file that we created back in Chapter 2 here. If you changed the name of the index that your data was uploaded to, modify this pipeline to reflect that change. Take a look at the Sigma rules that you will be using and do a spot check of the rules compared to the data you've got and make sure that the fields directly line up or that you've got a mapping written in your pipeline. Since I used Sysmon to capture this data, there should not be much of a deviation. 

Before we move on there is one last thing you need to check in the pipeline - make sure that the `postprocessing` section defines the `dispatch.index_earliest` field with some large value like `-1y`. I do not recommend doing this against production data, but in our instance we need to make sure that the small amount of data that I generated for this lab will actually trigger detections. By using `index_earliest` we are defining the timestamp that this data was written to the index which is opposed to the normal `_time` (and the associated `earliest_time` search modifier) field that you see in your logs which is when the event was generated on the endpoint.

Finally we need to review our cron task and make sure that it is running as expected. If you designed your cron task to run once every 24 hours, feel free to go touch some grass. If you're impatient you can just run the script manually and let your rules get populated inside of Splunk. Based on how you defined the `cron_schedule` in your pipeline for each rule you can again either go touch some grass or execute the some of the searches manually. (Settings -> Searches, reports, and alerts -> Set `App:` to our Sigma app and `Owner:` to all -> hit `Run`)

If your pipeline matches mine and it is using `| collect index=notable_events`, confirm that you have created the `notable_events` index before you run any rules. Once you've got detections firing you will see events in this index that you can use to alert your SOC of potential security events!

**TODO** Trigger the rules and show some instances from the logs I created