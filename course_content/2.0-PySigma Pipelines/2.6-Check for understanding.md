# 2.5 :: Check for Understanding

# TODO
- explain how alert suppression, cron_schedule, and dispatch works in savedsearches.conf and show documentation
- give an example of using a rule condition in a post-processing pipeline
- show the student an output of a bulk conversion


To practice building a post processing pipeline, take the following scenario and build a transformation that will output into `savedsearches.conf` format.

Imagine you have a dataset with the following log sources and their importance state:

| Log Source | Importance | Unique Fields |
|--|--|--|
| ps_script | High | host, scriptContent |
| driver_load | Low | computerName, Image |
| dns_query | Medium | host, domain |
| file_event | Low | hostname, fileName, action |

Different importance values should be used to define how frequently the searches execute and how far back the search looks against your dataset.

| Importance | Frequency | Earliest Time |
|--|--|--|
| High | every 5 min | last 10 min |
| Medium | every 30m | last hour |
| low | once at midnight | last 24 hours |

You have the following requirements:

- To avoid duplicate detections, you will also want to define the specific field name suppressions based on the specific log sources using the `alert.suppress.fields` object and `alert.suppress.period` to define how long to suppress the search based on the `dispatch.earliest_time` value you set (example: if `dispatch.earliest_time = -1h` then `alert.suppress.period = 1h`)
- To make this pipeline scalable to future log sources you might ingest, set some default values for frequency and the earliest time search that match the low importance log sources. For all searches, the latest time of each search should be set to `now`.

Hints:

- Use the splunk `savedsearches.conf` documentation as a reference.
- You will need to use `rule_conditions`.
- The `logsource` in this exercise refers to rules with a matching `category` value.
- Remember to set `alert.suppress` to `1` (true) or your suppression rules will not stick!
- Use a tool like crontab guru to validate your cron.
- Create a folder for the rules youâ€™d like to test by pulling a sample of the rules from the public repo that match these log sources and then validate your pipeline with the following command: `sigma convert -p <pipeline> -t splunk <rule_folder>` (beta.sigconverter.io is no longer sufficient here because you will be converting more than one rule at a time).