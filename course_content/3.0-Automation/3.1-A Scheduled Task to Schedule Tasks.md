# 3.1 :: A Scheduled Task to Schedule Tasks

We have an important mission in mind: automatically pulling rules from our GitHub repo and publishing them into Splunk as fully functional saved searches. To accomplish this goal, we are going use a cronjob to periodically perform a pull request against our repo, execute SigmaCLI with our pipelines, output the file into our savedsearches.conf, then force the Splunk server to refresh and update the loaded configuration. Up until this point we have done each of these steps manually with a simple script we can automate everything.

Before we build the script, we need to build a service account in Splunk. While there are definitely better ways to do this. We are going to include the username and password for the service account in plaintext in a script. This account should have limited permissions and the admin account you've been using up until this point would be a blatant violation of the rule of least privileges.

On the Splunk dashboard, navigate to the `Settings` page, select `users and authentication`, next the `Users` page, then the `Create User` feature. Follow the create user wizard and grant this account with privileges no greater than `user`. You can get granular if you'd like but for the sake of this course we will just keep it simple. Make sure you save the username and password you've created here.

With a service account in hand, we can create a bash script that will perform all of the steps we did above in a single go. Copy the below script, add the username and password you created in the curl command, and then save it somewhere meaningful:

```bash
#! /bin/bash

# Pull the git repo. If the repo is private, you'll need to do some extra steps here.
git -C ~/Sigma-Course/ pull
# Run the sigmaCLI command with your custom pipeline
sigma convert --target 'splunk' --pipeline ~/de-with-sigma/backend/pipelines/pipeline.yml ~/de-with-sigma/rules/production/ --skip-unsupported --output ~/de-with-sigma/backend/savedsearches.conf

# Move the savedsearches.conf output file to the splunk volume
mv ~/de-with-sigma/backend/savedsearches.conf ~/de-with-sigma/app/sigma/local/savedsearches.conf

# Hit the refresh endpoint for the app
curl -k -u <service account name>:<password> https://<Splunk Server IP>:8089/servicesNS/-/sigma/saved/searches/_reload
```
These are the same steps we ran in the previous section so they should ~just work~, but if you are unsure you can test. Move a rule into the production folder and then execute the script. Wait a minute and you will see it in the Splunk GUI.

Let's get this script scheduled. Run `crontab -l` and select your favorite editor. Append the following line to the end of the file (with your script location and .sh file name replaced and also where you'd like any logs to be saved):

```
0 0 * * * ~/<script location>/cronjob.sh >> ~/<perfered_log_location>/cron_log.txt
```

And that is it! You've got a scheduled task that, at midnight every day, will pull any new additions to your GitHub repo, automatically convert the rules into `savedsearches.conf` format, and update your Splunk server. **Mission Accomplished.**

